{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4 |Anaconda, Inc.| (default, Feb 19 2018, 10:59:04) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8de6a902e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import rectify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "nn = DenseLayer(l_states, 124, nonlinearity=rectify)\n",
    "nn = DenseLayer(nn, 64, nonlinearity=rectify)\n",
    "\n",
    "#output\n",
    "l_qvalues = DenseLayer(nn,num_units=n_actions,nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})\n",
    "get_qvalues = theano.function([current_states], T.argmax(predicted_qvalues, axis=1))\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_next_qvalues = get_output(l_qvalues,{l_states:next_states})\n",
    "\n",
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = rewards + gamma * T.max(predicted_next_qvalues, axis=1)\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean squared error loss function\n",
    "loss = lasagne.objectives.squared_error(predicted_qvalues_for_actions, target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.adam(loss.mean(),all_weights,learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states, actions, rewards, next_states, is_end],\n",
    "                             updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues(np.array([s],dtype=np.float32))[0] \n",
    "        rnd = np.random.uniform()\n",
    "        if rnd < epsilon:\n",
    "            a = np.random.choice(np.arange(n_actions))\n",
    "        else:\n",
    "            a = q_values\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step(np.array([s],dtype=np.float32),[a],[r],\n",
    "                   np.array([new_s],dtype=np.float32),[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:47<1:17:49, 47.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\tmean reward:-154.490\tepsilon:0.24750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [02:36<2:08:05, 78.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\tmean reward:-57.285\tepsilon:0.24502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [04:16<2:18:15, 85.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2\tmean reward:-1.836\tepsilon:0.24257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [06:31<2:36:46, 97.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3\tmean reward:-5.515\tepsilon:0.24015\n",
      "epoch:4\tmean reward:21.801\tepsilon:0.23775\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100)):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.99\n",
    "    print (\"epoch:%d\\tmean reward:%.3f\\tepsilon:%.5f\"%(i,np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 0:\n",
    "        print(\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.4",
   "language": "python",
   "name": "python354"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
