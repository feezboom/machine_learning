{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read data.\n",
    "linear_train = pd.read_csv('data/linear_train.txt', header=None).dropna()\n",
    "linear_ans_example = pd.read_csv('data/linear_ans_example.txt').dropna()\n",
    "linear_test = pd.read_csv('data/linear_test.txt', header=None).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_x = linear_train[0]\n",
    "full_y = linear_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def to_last_n_letters(array, n):\n",
    "    return [word[-(n*2):] for word in array]\n",
    "\n",
    "def append_hash_back(array):\n",
    "    return [word + \"#\" for word in array]\n",
    "\n",
    "def append_dollar_front(array):\n",
    "    return [\"$\" + word for word in array]\n",
    "\n",
    "def append_front_back(array):\n",
    "    return [\"$\" + word + \"#\" for word in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def encrypt_word(string, length):\n",
    "    return ' '.join(string[i:i+length] for i in range(0,len(string) - length,2))\n",
    "\n",
    "def encrypt_array_lengths(words_array, lengths):\n",
    "    return np.array([ ' '.join([encrypt_word(string, length) for length in lengths]) \n",
    "                     for string in words_array ])\n",
    "\n",
    "def isCapitalized(word):\n",
    "    capitals = ['А','Б','В','Г','Д','Е','Ё','Ж','З','И','Й','К','Л','М','Н','О',\n",
    "           'П','Р','С','Т','У','Ф','Х','Ц','Ч','Ш','Щ','Ъ','Ы','Ь','Э','Ю','Я']\n",
    "    return int(word[0:2] in capitals and not (word[2:4] in capitals))\n",
    "\n",
    "def vowel_count(word):\n",
    "    vowels = ['А','Е','Ё','И','О','У','Ы','Э','Ю','Я',\n",
    "              'а','е','ё','и','о','у','ы','э','ю','я']\n",
    "    retval = 0\n",
    "    for i in range (len(word)/2):\n",
    "        if word[i*2 : (i*2)+2] in vowels:\n",
    "            retval+=1\n",
    "    return retval\n",
    "\n",
    "def consonant_count(word):\n",
    "    consonants = ['Б','В','Г','Д','Ж','З','Й','К','Л','М','Н','П','Р','С','Т','Ф','Х','Ц','Ч','Ш','Щ','Ъ','Ь',\n",
    "                  'б','в','г','д','ж','з','й','к','л','м','н','п','р','с','т','ф','х','ц','ч','ш','щ','ъ','ь',]\n",
    "    retval = 0\n",
    "    for i in range (len(word)/2):\n",
    "        if word[i*2 : (i*2)+2] in consonants:\n",
    "            retval+=1\n",
    "    return retval\n",
    "    \n",
    "\n",
    "def cvect_fit_transform(count_vect, x, n_gram_count):\n",
    "    return count_vect.fit_transform(encrypt_array_lengths(x, n_gram_count*2))\n",
    "\n",
    "def cvect_transform(count_vect, x, n_gram_count):\n",
    "    return count_vect.transform(encrypt_array_lengths(x, n_gram_count*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_feature(functor,surnames_train, surnames_test, x_train, x_test):\n",
    "    first_capital_train = np.array([functor(word) for word in surnames_train]).reshape([-1,1])\n",
    "    x_train = hstack((x_train, coo_matrix(first_capital_train)))\n",
    "    \n",
    "    first_capital_test = np.array([functor(word) for word in surnames_test]).reshape([-1,1])\n",
    "    x_test = hstack((x_test, coo_matrix(first_capital_test)))\n",
    "    return (x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def write_to_csv(y, csv_name):\n",
    "    try :\n",
    "        os.mkdir(\"results\")\n",
    "    except:\n",
    "        pass\n",
    "    output = pd.DataFrame(data=y, columns=['Answer'])\n",
    "    output.index.name = 'Id'\n",
    "    output.to_csv(path_or_buf = './results/' + csv_name, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_result(clf, x_train_l, y_train_l, x_test_l):\n",
    "    count_vect = CountVectorizer(ngram_range=(1,1), decode_error='ignore', lowercase=True)\n",
    "    \n",
    "    x_train_new0 = count_vect.fit_transform(encrypt_array_lengths(x_train_l, np.array([3,4])*2))\n",
    "    x_test_new0 = count_vect.transform(encrypt_array_lengths(x_test_l, np.array([3,4])*2))\n",
    "    \n",
    "    x_train_new0, x_test_new0 = add_feature(isCapitalized, x_train_l, x_test_l, x_train_new0, x_test_new0)\n",
    "    x_train_new0, x_test_new0 = add_feature(vowel_count, x_train_l, x_test_l, x_train_new0, x_test_new0)\n",
    "    x_train_new0, x_test_new0 = add_feature(consonant_count, x_train_l, x_test_l, x_train_new0, x_test_new0)\n",
    "\n",
    "    x_train_new1 = count_vect.fit_transform(to_last_n_letters(x_train_l, 3))\n",
    "    x_test_new1 = count_vect.transform(to_last_n_letters(x_test_l, 3))\n",
    "                                  \n",
    "    x_train_new0, x_test_new0 = (hstack((x_train_new0, x_train_new1)), hstack((x_test_new0, x_test_new1)))\n",
    "                                  \n",
    "    clf = clf.fit(x_train_new0, y_train_l)\n",
    "    return clf.predict(x_test_new0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(full_x, full_y, train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833395014867\n",
      "CPU times: user 8.02 s, sys: 132 ms, total: 8.15 s\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(roc_auc_score(get_result(LogisticRegression(), append_front_back(x_train), y_train, append_front_back(x_test)), \n",
    "                     y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.718507513867\n",
      "CPU times: user 5.05 s, sys: 84 ms, total: 5.14 s\n",
      "Wall time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(roc_auc_score(get_result(MultinomialNB(), append_hash_back(x_train), y_train, append_hash_back(x_test)), \n",
    "                     y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 336 ms, total: 22 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = get_result(LogisticRegression(), linear_train[0], linear_train[1], linear_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write_to_csv(result, \"result0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Количество гласных/согласных - даёт небольшой буст ~ 0.5%\n",
    "# Окончания - добавление проверки сверху (поверх hash & dollar) ничего не даёт, кажется\n",
    "# Извлекать фичи SelectFromModel\n",
    "# Add hash to the end / beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Добавление символа в начало улучшило результат на 2% при 3,4 граммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def find_parameters(clf, x_train_l, y_train_l, x_test_l, y_test_l):\n",
    "    count_vect = CountVectorizer(ngram_range=(1,1), decode_error='ignore', lowercase=True)\n",
    "    \n",
    "    x_train_new0 = count_vect.fit_transform(encrypt_array_lengths(x_train_l, np.array([3,4])*2))\n",
    "    x_test_new0 = count_vect.transform(encrypt_array_lengths(x_test_l, np.array([3,4])*2))\n",
    "    \n",
    "    x_train_new0, x_test_new0 = add_feature(isCapitalized, x_train_l, x_test_l, x_train_new0, x_test_new0)\n",
    "    x_train_new0, x_test_new0 = add_feature(vowel_count, x_train_l, x_test_l, x_train_new0, x_test_new0)\n",
    "    x_train_new0, x_test_new0 = add_feature(consonant_count, x_train_l, x_test_l, x_train_new0, x_test_new0)\n",
    "\n",
    "    x_train_new1 = count_vect.fit_transform(to_last_n_letters(x_train_l, 3))\n",
    "    x_test_new1 = count_vect.transform(to_last_n_letters(x_test_l, 3))\n",
    "                                  \n",
    "    x_train_new0, x_test_new0 = (hstack((x_train_new0, x_train_new1)), hstack((x_test_new0, x_test_new1)))\n",
    "                               \n",
    "    clf_config = {           \n",
    "                'penalty' : ['l1', 'l2'], \n",
    "                'dual' : [False],\n",
    "                'max_iter' : np.arange(500,1501,500),\n",
    "                'tol' : [1e-4, 1e-5, 1e-6],\n",
    "                'C': [1, 10]\n",
    "              }\n",
    "    \n",
    "    scorer = make_scorer(score_func=roc_auc_score)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=clf, scoring=scorer, param_grid=clf_config, cv=5)\n",
    "    grid_search.fit(x_train_new0, y_train_l)\n",
    "        \n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grid_search.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test_l, grid_search.predict(x_test_new0)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "find_parameters(LogisticRegression(), x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
